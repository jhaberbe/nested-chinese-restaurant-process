{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b314fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scanpy anndata numpy pandas scipy statsmodels scikit-learn pyro-ppl torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86cfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"/home/jhaberbe/Projects/Personal/nested-chinese-restaurant-process/data/new_annotations.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdf6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor(adata[adata.obs[\"Cell.Subtype\"].eq(\"Astro\")].X.todense()).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2643e8",
   "metadata": {},
   "source": [
    "# Description of the sampling method\n",
    "\n",
    "1. Setup a tree structure\n",
    "- The intial node will be the full dataset, we fit a dirichlet multinomial distribution to the proposed dataset, and shove all the nodes into some index set. \n",
    "\n",
    "2. Begin the sampling procedure. \n",
    "- Given a random permutation of the nodes, we then sample each one sequentially. We take that node, and then ask whether to make a new node, or add it to a child that already exists. \n",
    "\n",
    "    - In the case that the current node has no children, this is trivial, we simply create a new node, and compute a new alpha for that node, use the code in `propose_new_alpha`\n",
    "\n",
    "    - If we have a set of children, then we do the following:\n",
    "        - calculate the log probabilities for each of the children, then compute the following update probabilities\n",
    "            - Existing node k: $\\log P_k = \\log(N_k) - \\log(\\alpha_n + \\sum_j N_j) + \\log P(x_i | \\text{child } k)$\n",
    "            - New node: $\\log P_{new} = \\log(\\alpha_n) - \\log(\\alpha_n + \\sum_j N_j) + \\log P(x_i | \\text{new child})$\n",
    "                - The probability of the new node is determined by some proposed alpha, which is a smoothing between the parent alpha and our new sample, which we just estimate using the following:\n",
    "                    - `alpha_new = alpha_parent * gamma + (1-gamma) * (x_0 + 1e-3)`\n",
    "        - We choose from the possible choices using a multinomial draw from the calculated probabilities. \n",
    "        - We continue this until we end up at a node with no children.\n",
    "\n",
    "3. Once we reach the end, we start from the top, but as we sample, we remove the index from the tree it was previously a part of. This will involve:\n",
    "- For nodes with >3 samples prior to removal, we remove the prior sample, and then refit the distribution using the remaining indices via minka's method\n",
    "- For nodes with 2 samples prior to removal, we remove the prior sample, and then refit using `propose_new_alpha`, but alpha_0 in this case will be the node's current alpha, instead of the parent alpha as it usually is. \n",
    "- If there was only one sample in the node, we remove the node. \n",
    "\n",
    "Continue until you're happy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce210ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pyro.distributions import DirichletMultinomial\n",
    "import random\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, indices, alpha, parent=None, vocab_size=None):\n",
    "        self.indices = set(indices)  # set of data indices in node\n",
    "        self.alpha = alpha  # torch tensor, Dirichlet concentration parameter (size vocab_size)\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def count(self, data_counts):\n",
    "        # Sum of counts over indices in this node\n",
    "        return data_counts[list(self.indices)].sum(dim=0)\n",
    "\n",
    "    def log_likelihood(self, x_i):\n",
    "        # Log prob of a single data point under this node's Dirichlet-Multinomial\n",
    "        dist = DirichletMultinomial(self.alpha, total_count = x_i.sum())\n",
    "        return dist.log_prob(x_i)\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "        child_node.parent = self\n",
    "\n",
    "    def remove_child(self, child_node):\n",
    "        self.children.remove(child_node)\n",
    "        child_node.parent = None\n",
    "\n",
    "    def remove_index(self, i):\n",
    "        self.indices.remove(i)\n",
    "\n",
    "    def add_index(self, i):\n",
    "        self.indices.add(i)\n",
    "\n",
    "\n",
    "def propose_new_alpha(alpha_parent, x_i, gamma=0.9, epsilon=1e-3):\n",
    "    return gamma * alpha_parent + (1 - gamma) * (x_i.float() + epsilon)\n",
    "\n",
    "def minka_estimate_alpha(X, max_iter=1000, tol=1e-3, eps=1e-10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Minka-style fixed-point update for Dirichlet-Multinomial fitting.\n",
    "\n",
    "    Args:\n",
    "        X: (N x K) torch tensor of count data\n",
    "        Returns: (K,) torch tensor of Dirichlet parameters alpha\n",
    "    \"\"\"\n",
    "    X = X.to(device)\n",
    "    N, K = X.shape\n",
    "    alpha = torch.ones(K, device=device)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        alpha_old = alpha.clone()\n",
    "        alpha_0 = alpha.sum()\n",
    "        digamma_alpha_k = digamma(alpha)\n",
    "        digamma_alpha_0 = digamma(alpha_0)\n",
    "\n",
    "        numerator = torch.sum(digamma(X + alpha) - digamma_alpha_k, dim=0)\n",
    "        denom = torch.sum(digamma(X.sum(dim=1, keepdim=True) + alpha_0) - digamma_alpha_0)\n",
    "\n",
    "        alpha = alpha * numerator / denom\n",
    "        alpha = torch.clamp(alpha, min=eps)\n",
    "\n",
    "        if torch.max(torch.abs(alpha - alpha_old)) < tol:\n",
    "            break\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def sample_child_node_or_new(parent_node, x_i, alpha_n, data_counts, gamma=0.9, epsilon=1e-3):\n",
    "    \"\"\"Given a parent node and data point x_i, sample which child to assign x_i to or create new.\"\"\"\n",
    "\n",
    "    N_js = torch.tensor([len(child.indices) for child in parent_node.children], dtype=torch.float)\n",
    "    sum_N_js = N_js.sum()\n",
    "    log_probs = []\n",
    "\n",
    "    # Existing children\n",
    "    for child in parent_node.children:\n",
    "        log_p_k = torch.log(torch.tensor(len(child.indices), dtype=torch.float32)) - torch.log(torch.tensor(alpha_n + sum_N_js, dtype=torch.float32)) + child.log_likelihood(x_i)\n",
    "        log_probs.append(log_p_k)\n",
    "\n",
    "    # New child\n",
    "    alpha_new = propose_new_alpha(parent_node.alpha, x_i, gamma=gamma, epsilon=epsilon)\n",
    "    new_node = TreeNode(indices=set(), alpha=alpha_new, vocab_size=parent_node.vocab_size)\n",
    "    log_p_new = torch.log(torch.tensor(alpha_n, dtype=torch.float32)) - torch.log(torch.tensor(alpha_n + sum_N_js, dtype=torch.float32)) + new_node.log_likelihood(x_i)\n",
    "    log_probs.append(log_p_new)\n",
    "\n",
    "    log_probs_tensor = torch.stack(log_probs)\n",
    "    probs = torch.softmax(log_probs_tensor, dim=0)\n",
    "    choice = torch.multinomial(probs, 1).item()\n",
    "\n",
    "    if choice == len(parent_node.children):\n",
    "        # Create new child node\n",
    "        parent_node.add_child(new_node)\n",
    "        return new_node\n",
    "    else:\n",
    "        return parent_node.children[choice]\n",
    "\n",
    "def prune_tree(node, min_size):\n",
    "    # Copy list to avoid modification during iteration\n",
    "    children_copy = node.children[:]\n",
    "    \n",
    "    for child in children_copy:\n",
    "        prune_tree(child, min_size)  # recurse first\n",
    "        \n",
    "        # If child is too small, remove it\n",
    "        if len(child.indices) < min_size:\n",
    "            node.remove_child(child)\n",
    "            # Optionally, you might want to reassign child.indices to parent or handle them somehow\n",
    "            # For example, add back to parent node indices if appropriate:\n",
    "            # node.indices.update(child.indices)\n",
    "\n",
    "def gibbs_sample(data_counts, root_node, alpha_n, gamma=0.9, epsilon=1e-3, n_iters=10):\n",
    "    \"\"\"\n",
    "    data_counts: tensor shape (N_samples, V)\n",
    "    root_node: initial root TreeNode with all indices assigned\n",
    "    alpha_n: scalar CRP concentration parameter\n",
    "    \"\"\"\n",
    "\n",
    "    N = data_counts.shape[0]\n",
    "    all_indices = list(range(N))\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        random.shuffle(all_indices)\n",
    "        for i in tqdm(all_indices):\n",
    "            # Remove i from tree\n",
    "            node_i = find_node_containing_index(root_node, i)  # You need to implement this tree traversal\n",
    "            if node_i is None:\n",
    "                continue  # Shouldn't happen\n",
    "\n",
    "            if node_i is root_node:\n",
    "                pass\n",
    "            else:\n",
    "                # Remove i and update node alpha\n",
    "                if len(node_i.indices) > 3:\n",
    "                    node_i.remove_index(i)\n",
    "                    subset_counts = data_counts[list(node_i.indices)]\n",
    "                    node_i.alpha = minka_estimate_alpha(subset_counts)\n",
    "                elif len(node_i.indices) == 2:\n",
    "                    node_i.remove_index(i)\n",
    "                    node_i.alpha = propose_new_alpha(node_i.alpha, data_counts[i], gamma=gamma, epsilon=epsilon)\n",
    "                else:  # len == 1\n",
    "                    node_i.remove_index(i)\n",
    "                    # Remove node from tree\n",
    "                    if node_i.parent is not None:\n",
    "                        node_i.parent.remove_child(node_i)\n",
    "\n",
    "            # Sample new assignment\n",
    "            current_node = root_node\n",
    "            while True:\n",
    "                if len(current_node.children) == 0:\n",
    "                    # Create new node\n",
    "                    new_alpha = propose_new_alpha(current_node.alpha, data_counts[i], gamma=gamma, epsilon=epsilon)\n",
    "                    new_node = TreeNode(indices={i}, alpha=new_alpha, parent=current_node, vocab_size=current_node.vocab_size)\n",
    "                    current_node.add_child(new_node)\n",
    "                    break\n",
    "                else:\n",
    "                    next_node = sample_child_node_or_new(current_node, data_counts[i], alpha_n, data_counts, gamma=gamma, epsilon=epsilon)\n",
    "                    if len(next_node.children) == 0:\n",
    "                        next_node.add_index(i)\n",
    "                        break\n",
    "                    else:\n",
    "                        current_node = next_node\n",
    "        prune_tree(root_node, min_size=data_counts.shape[0]//100)\n",
    "    return root_node\n",
    "\n",
    "\n",
    "def find_node_containing_index(node, i):\n",
    "    \"\"\"Recursive search for node containing index i\"\"\"\n",
    "    if i in node.indices:\n",
    "        return node\n",
    "    for child in node.children:\n",
    "        found = find_node_containing_index(child, i)\n",
    "        if found:\n",
    "            return found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb075d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_root = minka_estimate_alpha(X[:500])\n",
    "root_node = TreeNode(indices=set(range(500)), alpha=alpha_root, vocab_size=X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "900dd011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/tmp/ipykernel_3217099/345420060.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_p_k = torch.log(torch.tensor(len(child.indices), dtype=torch.float32)) - torch.log(torch.tensor(alpha_n + sum_N_js, dtype=torch.float32)) + child.log_likelihood(x_i)\n",
      "/tmp/ipykernel_3217099/345420060.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_p_new = torch.log(torch.tensor(alpha_n, dtype=torch.float32)) - torch.log(torch.tensor(alpha_n + sum_N_js, dtype=torch.float32)) + new_node.log_likelihood(x_i)\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.53it/s]\n",
      "100%|██████████| 500/500 [01:11<00:00,  6.98it/s]\n",
      "100%|██████████| 500/500 [01:37<00:00,  5.14it/s]\n",
      "100%|██████████| 500/500 [02:00<00:00,  4.16it/s]\n",
      " 11%|█         | 54/500 [00:14<01:57,  3.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m root_node = \u001b[43mgibbs_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mgibbs_sample\u001b[39m\u001b[34m(data_counts, root_node, alpha_n, gamma, epsilon, n_iters)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     next_node = \u001b[43msample_child_node_or_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(next_node.children) == \u001b[32m0\u001b[39m:\n\u001b[32m    145\u001b[39m         next_node.add_index(i)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36msample_child_node_or_new\u001b[39m\u001b[34m(parent_node, x_i, alpha_n, data_counts, gamma, epsilon)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Existing children\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m parent_node.children:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     log_p_k = torch.log(torch.tensor(\u001b[38;5;28mlen\u001b[39m(child.indices), dtype=torch.float32)) - torch.log(torch.tensor(alpha_n + sum_N_js, dtype=torch.float32)) + \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     log_probs.append(log_p_k)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# New child\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mTreeNode.log_likelihood\u001b[39m\u001b[34m(self, x_i)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_i):\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Log prob of a single data point under this node's Dirichlet-Multinomial\u001b[39;00m\n\u001b[32m     19\u001b[39m     dist = DirichletMultinomial(\u001b[38;5;28mself\u001b[39m.alpha, total_count = x_i.sum())\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/nested-chinese-restaurant-process/.venv/lib/python3.13/site-packages/pyro/distributions/conjugate.py:212\u001b[39m, in \u001b[36mDirichletMultinomial.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_sample(value)\n\u001b[32m    211\u001b[39m alpha = \u001b[38;5;28mself\u001b[39m.concentration\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_log_beta_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_sparse\u001b[49m\u001b[43m)\u001b[49m - _log_beta_1(\n\u001b[32m    213\u001b[39m     alpha, value, \u001b[38;5;28mself\u001b[39m.is_sparse\n\u001b[32m    214\u001b[39m ).sum(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/nested-chinese-restaurant-process/.venv/lib/python3.13/site-packages/pyro/distributions/conjugate.py:30\u001b[39m, in \u001b[36m_log_beta_1\u001b[39m\u001b[34m(alpha, value, is_sparse)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         torch.lgamma(\u001b[32m1\u001b[39m + value) + torch.lgamma(alpha) - \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlgamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "root_node = gibbs_sample(X[:500], root_node, alpha_n=1.0, gamma=0.9, epsilon=1e-3, n_iters=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
